# GPT-2 Text Generator & Fine-Tuning

This repository provides a simple implementation of **text generation** and **fine-tuning** using [HuggingFace Transformers](https://huggingface.co/transformers/) with GPT-2.  
It includes:
- Text generation with sampling (`top-k`, `top-p`, `temperature`).
- Fine-tuning GPT-2 on custom training texts.
- Measuring generation time for benchmarking.

---

## ðŸš€ Features
- Generate new text based on a given prompt.
- Fine-tune GPT-2 with your own dataset.
- Benchmark generation speed.
- Support for CUDA (GPU) if available.
